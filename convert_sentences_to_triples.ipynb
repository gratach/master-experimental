{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gratach/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from json import loads, dumps\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "nltk.download('punkt')\n",
    "datapath = Path(\"../master-database-files/master-experimental/convert_sentences_to_triples\")\n",
    "assert datapath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikipediaText(title):\n",
    "    return [*loads(get(f\"https://en.wikipedia.org/w/api.php?action=query&format=json&titles={title}&prop=extracts&explaintext\").text)[\"query\"][\"pages\"].items()][0][1][\"extract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60365"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electronText = getWikipediaText(\"Electron\")\n",
    "electronSentences = sent_tokenize(electronText)\n",
    "(datapath/ \"sentences.json\").write_text(dumps(electronSentences, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from pathlib import Path\n",
    "from json import loads, dumps\n",
    "from random import choice\n",
    "environ[\"OPENAI_API_KEY\"] = Path(\"~/.openaiapikey\").expanduser().read_text().strip()\n",
    "\n",
    "from openai import OpenAI\n",
    "from random import randint\n",
    "\n",
    "openaiClient = OpenAI()\n",
    "def gpt_3_5_turbo_completion(query, temperature = 1):\n",
    "    answer = openaiClient.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        temperature = temperature,\n",
    "        seed = randint(0, 1000000)\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "def gpt_4o_mini_completion(query, temperature = 1):\n",
    "    answer = openaiClient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        temperature = temperature,\n",
    "        seed = randint(0, 1000000)\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "\n",
    "def gpt_4_turbo_completion(query, temperature = 1):\n",
    "    answer = openaiClient.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        temperature = temperature,\n",
    "        seed = randint(0, 1000000)\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "\n",
    "def tryRecieveAnswer(query, completionFunction = gpt_3_5_turbo_completion, answerConversion = lambda x: True, maxTries = 10, temperature = 1):\n",
    "    tryNumber = 0\n",
    "    while tryNumber < maxTries:\n",
    "        answer = completionFunction(query, temperature)\n",
    "        try:\n",
    "            answer = answerConversion(answer)\n",
    "            return (answer, True)\n",
    "        except:\n",
    "            pass\n",
    "        tryNumber += 1\n",
    "    print(f\"Failed to recieve answer for query: {query}\")\n",
    "    return (None, False)\n",
    "\n",
    "def listAnswerConversion(answer):\n",
    "    result = loads(answer)\n",
    "    assert isinstance(result, list)\n",
    "    for item in result:\n",
    "        assert isinstance(item, str)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not usefull\n",
    "ef resolveSentences(sourcepath, targetpath):\n",
    "    unresolvedSentences = loads(sourcepath.read_text())\n",
    "    resolvedSentences = [] if not targetpath.exists() else loads(targetpath.read_text())\n",
    "    while len(resolvedSentences) < len(unresolvedSentences):\n",
    "        sentence = unresolvedSentences[len(resolvedSentences)]\n",
    "        contextSentence = resolvedSentences[-1] if len(resolvedSentences) > 0 else \"\"\n",
    "        query = f'''\n",
    "Example: The sentence \"He gave her a book and she read it.\" with the context sentence \"John met Merry in the school\" can be resolved to \"John gave Merry a book and Marry read the book\". \n",
    "Please resolve the coreferences in the sentence: \"{sentence}\" with the context sentence: \"{contextSentence}\". Return only the sentence (not the context sentence) with the coreference resolved quoted in \"\".'''\n",
    "        def answerConversion(answer):\n",
    "            assert answer.startswith('\"') and answer.endswith('\"')\n",
    "            return answer[1:-1]\n",
    "        (answer, success) = tryRecieveAnswer(query, answerConversion=answerConversion)\n",
    "        if success:\n",
    "            resolvedSentences.append(answer)\n",
    "            targetpath.write_text(dumps(resolvedSentences, indent=2))\n",
    "        else:\n",
    "            resolvedSentences.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyUsfullSentences(sourcepath, evaluationpath, targetpath):\n",
    "    sentences = loads(sourcepath.read_text())\n",
    "    evaluations = [] if not evaluationpath.exists() else loads(evaluationpath.read_text())\n",
    "    while len(evaluations) < len(sentences):\n",
    "        sentence = sentences[len(evaluations)]\n",
    "        query = f'''\n",
    "Is the following text a single, grammatically correct sentence that does not refer to anything outside of the text?\n",
    "BEGINNING OF TEXT\n",
    "{sentence}\n",
    "END OF TEXT\n",
    "Answer only with \"y\" or \"n\".'''\n",
    "        def answerConversion(answer):\n",
    "            assert answer in [\"y\", \"n\"]\n",
    "            return answer\n",
    "        (answer, success) = tryRecieveAnswer(query, answerConversion=answerConversion, completionFunction=gpt_4o_mini_completion)\n",
    "        if success:\n",
    "            evaluations.append(answer)\n",
    "            evaluationpath.write_text(dumps(evaluations, indent=2))\n",
    "        else:\n",
    "            evaluations.append(\"n\")\n",
    "    targetpath.write_text(dumps([sentence for i, sentence in enumerate(sentences) if evaluations[i] == \"y\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifyUsfullSentences(datapath/ \"sentences.json\", datapath/ \"sentences-evaluations.json\", datapath/ \"usefull-sentences.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSemanticTriplesFromSentence(sentence):\n",
    "    query = f'''\n",
    "The sentence \"The Earth orbits the sun at a distance of 1 AU.\" can be converted to the semantic triples:\n",
    "[\n",
    "  [\"Earth\", \"orbits\", \"Sun\"],\n",
    "  [\"Earth\", \"has a distance from the sun of\", \"1 AU\"]\n",
    "]\n",
    "What are the semantic triples for the sentence: \"{sentence}\"?\n",
    "Return nothing but the semantic triples in the format above.'''\n",
    "    def answerConversion(answer):\n",
    "        ret = loads(answer)\n",
    "        assert isinstance(ret, list)\n",
    "        for triple in ret:\n",
    "            assert isinstance(triple, list) and len(triple) == 3\n",
    "            assert all(isinstance(item, str) for item in triple)\n",
    "        return ret\n",
    "    (answer, success) = tryRecieveAnswer(query, answerConversion=answerConversion)\n",
    "    if success:\n",
    "        return answer\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Electrons', 'belong to', 'first generation of lepton particle family'],\n",
       " ['Electrons', 'are generally thought to be', 'elementary particles'],\n",
       " ['Electrons', 'have', 'no known components or substructure']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractSemanticTriplesFromSentence(\"Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSentencesToSemanticTriples(sourcepath, targetpath):\n",
    "    sentences = loads(sourcepath.read_text())\n",
    "    triples = [] if not targetpath.exists() else loads(targetpath.read_text())\n",
    "    while len(triples) < len(sentences):\n",
    "        sentence = sentences[len(triples)]\n",
    "        triples.append(extractSemanticTriplesFromSentence(sentence))\n",
    "        targetpath.write_text(dumps(triples, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertSentencesToSemanticTriples(datapath/ \"usefull-sentences.json\", datapath/ \"semantic-triples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSemanticTriplesToSentence(triples):\n",
    "    query = f'''\n",
    "The semantic triples\n",
    "[\n",
    "  [\"Earth\", \"orbits\", \"Sun\"],\n",
    "  [\"Earth\", \"has a distance from the sun of\", \"1 AU\"]\n",
    "]\n",
    "can be combined to the sentence \"The Earth orbits the sun at a distance of 1 AU.\".\n",
    "What is the sentence for the semantic triples:\n",
    "{dumps(triples, indent=2)}\n",
    "Return nothing but the sentence quoted in \"\".'''\n",
    "    def answerConversion(answer):\n",
    "        assert answer.startswith('\"') and answer.endswith('\"')\n",
    "        return answer[1:-1]\n",
    "    (answer, success) = tryRecieveAnswer(query, answerConversion=answerConversion)\n",
    "    if success:\n",
    "        return answer\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Electrons exhibit properties of both particles and waves, can collide with other particles, and can be diffracted like light.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combineSemanticTriplesToSentence([\n",
    "    [\n",
    "      \"electrons\",\n",
    "      \"exhibit\",\n",
    "      \"properties of particles\"\n",
    "    ],\n",
    "    [\n",
    "      \"electrons\",\n",
    "      \"exhibit\",\n",
    "      \"properties of waves\"\n",
    "    ],\n",
    "    [\n",
    "      \"electrons\",\n",
    "      \"can collide with\",\n",
    "      \"other particles\"\n",
    "    ],\n",
    "    [\n",
    "      \"electrons\",\n",
    "      \"can be diffracted like\",\n",
    "      \"light\"\n",
    "    ]\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSemanticTriplesToSentences(sourcepath, targetpath):\n",
    "    triples = loads(sourcepath.read_text())\n",
    "    sentences = [] if not targetpath.exists() else loads(targetpath.read_text())\n",
    "    while len(sentences) < len(triples):\n",
    "        sentenceTriples = triples[len(sentences)]\n",
    "        sentences.append(combineSemanticTriplesToSentence(sentenceTriples))\n",
    "        targetpath.write_text(dumps(sentences, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertSemanticTriplesToSentences(datapath/ \"semantic-triples.json\", datapath/ \"converted-sentences.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareStringLengths(firstpath, secondpath, resultpath):\n",
    "    first = loads(firstpath.read_text())\n",
    "    second = loads(secondpath.read_text())\n",
    "    assert len(first) == len(second)\n",
    "    lengthDifferences = [len(second[i]) - len(first[i]) for i in range(len(first))]\n",
    "    averageLengthFirst = sum(len(sentence) for sentence in first) / len(first)\n",
    "    averageLengthFirstError = (sum((len(sentence) - averageLengthFirst) ** 2 for sentence in first) / (len(first) * (len(first) - 1))) ** 0.5\n",
    "    averageLengthSecond = sum(len(sentence) for sentence in second) / len(second)\n",
    "    averageLengthSecondError = (sum((len(sentence) - averageLengthSecond) ** 2 for sentence in second) / (len(second) * (len(second) - 1))) ** 0.5\n",
    "    averageLengthDifference = sum(lengthDifference for lengthDifference in lengthDifferences) / len(lengthDifferences)\n",
    "    averageLengthDifferenceError = (sum((lengthDifference - averageLengthDifference) ** 2 for lengthDifference in lengthDifferences) / (len(lengthDifferences) * (len(lengthDifferences) - 1))) ** 0.5\n",
    "    lengthChangePercentage = averageLengthDifference / averageLengthFirst * 100\n",
    "    lengthChangePercentageError = ((averageLengthDifferenceError / averageLengthFirst)**2 + (averageLengthFirstError * averageLengthDifference / averageLengthFirst**2)**2)**0.5 * 100\n",
    "    resultpath.write_text(dumps({\n",
    "        \"averageLengthFirst\": averageLengthFirst,\n",
    "        \"averageLengthFirstError\": averageLengthFirstError,\n",
    "        \"averageLengthSecond\": averageLengthSecond,\n",
    "        \"averageLengthSecondError\": averageLengthSecondError,\n",
    "        \"averageLengthDifference\": averageLengthDifference,\n",
    "        \"averageLengthDifferenceError\": averageLengthDifferenceError,\n",
    "        \"lengthChangePercentage\": lengthChangePercentage,\n",
    "        \"lengthChangePercentageError\": lengthChangePercentageError\n",
    "        }, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareStringLengths(datapath/ \"usefull-sentences.json\", datapath/ \"converted-sentences.json\", datapath/ \"length-comparison.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostUsedNewAndOldWords(firstpath, secondpath, changedWordsPath):\n",
    "    first = loads(firstpath.read_text())\n",
    "    second = loads(secondpath.read_text())\n",
    "    changedWords = {}\n",
    "    for oldSentence, newSentence in zip(first, second):\n",
    "        oldWordsInSentence = set(oldSentence.lower().replace(\".\", \" \").replace(\",\", \" \").split())\n",
    "        newWordsInSentence = set(newSentence.lower().replace(\".\", \" \").replace(\",\", \" \").split())\n",
    "        for word in newWordsInSentence:\n",
    "            if word in changedWords:\n",
    "                changedWords[word] += 1\n",
    "            else:\n",
    "                changedWords[word] = 1\n",
    "        for word in oldWordsInSentence:\n",
    "            if word in changedWords:\n",
    "                changedWords[word] -= 1\n",
    "            else:\n",
    "                changedWords[word] = -1\n",
    "    # Sort the words by their frequency\n",
    "    changedWords = sorted(changedWords.items(), key=lambda item: item[1], reverse=True)\n",
    "    changedWordsPath.write_text(dumps(changedWords, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMostUsedNewAndOldWords(datapath/ \"usefull-sentences.json\", datapath/ \"converted-sentences.json\", datapath/ \"changed-words.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTriplesStatistics(triplesPath, statisticsPath):\n",
    "    triples = loads(triplesPath.read_text())\n",
    "    triplesCount = len(triples)\n",
    "    triplesLengths = [len(triple) for triple in triples]\n",
    "    averageTriplesLength = sum(triplesLengths) / triplesCount\n",
    "    averageTriplesLengthError = (sum((length - averageTriplesLength) ** 2 for length in triplesLengths) / (triplesCount * (triplesCount - 1))) ** 0.5\n",
    "    totalNumberOfTriples = sum(triplesLengths)\n",
    "    statisticsPath.write_text(dumps({\n",
    "        \"triplesCount\": triplesCount,\n",
    "        \"averageTriplesLength\": averageTriplesLength,\n",
    "        \"averageTriplesLengthError\": averageTriplesLengthError,\n",
    "        \"totalNumberOfTriples\": totalNumberOfTriples\n",
    "    }, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateTriplesStatistics(datapath/ \"semantic-triples.json\", datapath/ \"triples-statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostUsedTripleParts(triplesPath, mostUsedPartsPath):\n",
    "    triples = loads(triplesPath.read_text())\n",
    "    subjects = {}\n",
    "    predicates = {}\n",
    "    objects = {}\n",
    "    for triple in triples:\n",
    "        for part in triple:\n",
    "            if part[0].lower() in subjects:\n",
    "                subjects[part[0].lower()] += 1\n",
    "            else:\n",
    "                subjects[part[0].lower()] = 1\n",
    "            if part[1].lower() in predicates:\n",
    "                predicates[part[1].lower()] += 1\n",
    "            else:\n",
    "                predicates[part[1].lower()] = 1\n",
    "            if part[2].lower() in objects:\n",
    "                objects[part[2].lower()] += 1\n",
    "            else:\n",
    "                objects[part[2].lower()] = 1\n",
    "    # Sort the parts by their frequency\n",
    "    subjects = sorted(subjects.items(), key=lambda item: item[1], reverse=True)\n",
    "    predicates = sorted(predicates.items(), key=lambda item: item[1], reverse=True)\n",
    "    objects = sorted(objects.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Use only those parts that are used more than once\n",
    "    subjects = [subject for subject in subjects if subject[1] > 1]\n",
    "    predicates = [predicate for predicate in predicates if predicate[1] > 1]\n",
    "    objects = [object for object in objects if object[1] > 1]\n",
    "    mostUsedPartsPath.write_text(dumps({\n",
    "        \"subjects\": subjects,\n",
    "        \"predicates\": predicates,\n",
    "        \"objects\": objects\n",
    "    }, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMostUsedTripleParts(datapath/ \"semantic-triples.json\", datapath/ \"most-used-parts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAveragePredicatesWordLength(triplesPath, averageWordLengthPath):\n",
    "    triplegroups = loads(triplesPath.read_text())\n",
    "    predicates = [triple[1] for triplegroup in triplegroups for triple in triplegroup]\n",
    "    predicatesWordLengths = [len(predicate.split()) for predicate in predicates]\n",
    "    averageWordLength = sum(predicatesWordLengths) / len(predicatesWordLengths)\n",
    "    averageWordLengthError = (sum((length - averageWordLength) ** 2 for length in predicatesWordLengths) / (len(predicatesWordLengths) * (len(predicatesWordLengths) - 1))) ** 0.5\n",
    "    averageWordLengthPath.write_text(dumps({\n",
    "        \"averageWordLength\": averageWordLength,\n",
    "        \"averageWordLengthError\": averageWordLengthError\n",
    "    }, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateAveragePredicatesWordLength(datapath/ \"semantic-triples.json\", datapath/ \"average-word-length.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
